{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4157af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and logging configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.metrics import ContextRelevance\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "\n",
    "import logging, sys, nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"ragas\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# Set OpenAI API Key (ensure it's in your environment or set it here)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# Database connection string\n",
    "raw_db_url = os.environ.get(\"DATABASE_URL\", \"postgresql://postgres:postgres@localhost:5432/rag_viz\")\n",
    "if \"?\" in raw_db_url:\n",
    "    DB_URL = raw_db_url.split(\"?\")[0]\n",
    "else:\n",
    "    DB_URL = raw_db_url\n",
    "\n",
    "print(\"Libraries imported and logging configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07674270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database error: connection to server at \"127.0.0.1\", port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Computer\\AppData\\Local\\Temp\\ipykernel_20568\\3712908999.py\", line 3, in <module>\n",
      "    conn = psycopg2.connect(DB_URL)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\projects\\547-vis-project\\.venv312\\Lib\\site-packages\\psycopg2\\__init__.py\", line 135, in connect\n",
      "    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "psycopg2.OperationalError: connection to server at \"127.0.0.1\", port 5432 failed: Connection refused (0x0000274D/10061)\n",
      "\tIs the server running on that host and accepting TCP/IP connections?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Connect to Database and Fetch Data\n",
    "try:\n",
    "    conn = psycopg2.connect(DB_URL)\n",
    "    cur = conn.cursor(cursor_factory=RealDictCursor)\n",
    "    \n",
    "    # Fetch Answers with Questions and Retrievals from JSON field\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            a.id as answer_id,\n",
    "            q.text as question,\n",
    "            a.text as answer,\n",
    "            a.retrievals,\n",
    "            a.\"llmScore\"\n",
    "        FROM \"Answer\" a\n",
    "        JOIN \"Question\" q ON a.\"questionId\" = q.id\n",
    "        WHERE a.retrievals IS NOT NULL AND jsonb_array_length(a.retrievals::jsonb) > 0\n",
    "    \"\"\"\n",
    "    \n",
    "    cur.execute(query)\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    print(f\"Fetched {len(df)} records with retrievals.\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"WARNING: No records found with retrievals. Make sure populate_retrievals.ts has been run.\")\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "    else:\n",
    "        # Extract contexts from the retrievals JSON\n",
    "        # Each retrieval has: {id, score, text, documentTitle, index}\n",
    "        # Ragas expects 'contexts' to be a list of strings (the text field)\n",
    "        df['contexts'] = df['retrievals'].apply(\n",
    "            lambda x: [r['text'] for r in x] if x and isinstance(x, list) else []\n",
    "        )\n",
    "        \n",
    "        # Select only the columns needed for RAGAS\n",
    "        # Ragas standard columns: question, answer, contexts\n",
    "        df = df[['answer_id', 'question', 'answer', 'contexts', 'llmScore']]\n",
    "        \n",
    "        print(f\"Sample record:\")\n",
    "        print(f\"  Question: {df.iloc[0]['question'][:100]}...\")\n",
    "        print(f\"  Answer: {df.iloc[0]['answer'][:100]}...\")\n",
    "        print(f\"  Contexts: {len(df.iloc[0]['contexts'])} chunks\")\n",
    "        print(f\"  Current llmScore: {df.iloc[0]['llmScore']}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "251aee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Ollama model: qwen3:8b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Computer\\AppData\\Local\\Temp\\ipykernel_20568\\3034598362.py:23: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  wrapped = LangchainLLMWrapper(m)\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'qwen3:8b' responded: To provide a helpful health check, could you clarify which area you're referring...\n",
      "Prepared OpenAI judge model: gpt-4o-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Computer\\AppData\\Local\\Temp\\ipykernel_20568\\3034598362.py:36: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  openai_judge_wrapper = LangchainLLMWrapper(openai_judge)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS metrics configured using judge: ollama:qwen3:8b\n",
      "OpenAI judge fallback enabled (AUTO_FALLBACK_OPENAI=1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Computer\\AppData\\Local\\Temp\\ipykernel_20568\\3034598362.py:53: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  embeddings = LangchainEmbeddingsWrapper(openai_embeddings)\n"
     ]
    }
   ],
   "source": [
    "# Configure RAGAS with Ollama (LLM Judge) and OpenAI Embeddings\n",
    "# Adds OpenAI fallback (gpt-4o-mini) if Ollama model fails or env overrides.\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    LLMContextPrecisionWithoutReference,\n",
    ")\n",
    "\n",
    "# Preferred order of local fallback models (Ollama)\n",
    "CANDIDATE_MODELS = [\"qwen3:8b\"]\n",
    "MODEL_NAME = CANDIDATE_MODELS[0]\n",
    "\n",
    "USE_OPENAI_JUDGE = os.getenv(\"USE_OPENAI_JUDGE\", \"0\") == \"1\"  # force OpenAI judge\n",
    "AUTO_FALLBACK_OPENAI = os.getenv(\"AUTO_FALLBACK_OPENAI\", \"1\") == \"1\"  # retry with OpenAI on failure\n",
    "OPENAI_JUDGE_MODEL = os.getenv(\"OPENAI_JUDGE_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "# Build Ollama-based judge\n",
    "def build_ollama_llm(model_name: str):\n",
    "    print(f\"Initializing Ollama model: {model_name}\")\n",
    "    m = ChatOllama(model=model_name, temperature=0)\n",
    "    wrapped = LangchainLLMWrapper(m)\n",
    "    try:\n",
    "        probe = m.invoke(\"Health check.\")\n",
    "        print(f\"Model '{model_name}' responded: {probe.content[:80]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model '{model_name}' failed probe: {e}\")\n",
    "    return m, wrapped\n",
    "\n",
    "ollama_model, ollama_llm_wrapper = build_ollama_llm(MODEL_NAME)\n",
    "\n",
    "# Build OpenAI judge (always available if key set) for fallback or forced usage\n",
    "try:\n",
    "    openai_judge = ChatOpenAI(model=OPENAI_JUDGE_MODEL, temperature=0)\n",
    "    openai_judge_wrapper = LangchainLLMWrapper(openai_judge)\n",
    "    print(f\"Prepared OpenAI judge model: {OPENAI_JUDGE_MODEL}\")\n",
    "except Exception as e:\n",
    "    openai_judge = None\n",
    "    openai_judge_wrapper = None\n",
    "    print(f\"‚ö†Ô∏è Could not initialize OpenAI judge: {e}\")\n",
    "\n",
    "# Choose primary llm wrapper\n",
    "if USE_OPENAI_JUDGE and openai_judge_wrapper is not None:\n",
    "    llm = openai_judge_wrapper\n",
    "    ACTIVE_JUDGE = f\"openai:{OPENAI_JUDGE_MODEL} (forced)\"\n",
    "else:\n",
    "    llm = ollama_llm_wrapper\n",
    "    ACTIVE_JUDGE = f\"ollama:{MODEL_NAME}\"\n",
    "\n",
    "# Embeddings (ensure OPENAI_API_KEY set)\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "embeddings = LangchainEmbeddingsWrapper(openai_embeddings)\n",
    "\n",
    "# Metrics\n",
    "context_precision_no_ref = LLMContextPrecisionWithoutReference()\n",
    "metrics = [faithfulness, answer_relevancy, context_precision_no_ref]\n",
    "for metric in metrics:\n",
    "    if hasattr(metric, \"llm\"):\n",
    "        metric.llm = llm\n",
    "    if hasattr(metric, \"embeddings\"):\n",
    "        metric.embeddings = embeddings\n",
    "\n",
    "print(f\"RAGAS metrics configured using judge: {ACTIVE_JUDGE}\")\n",
    "if not USE_OPENAI_JUDGE and openai_judge_wrapper is not None:\n",
    "    print(\"OpenAI judge fallback enabled (AUTO_FALLBACK_OPENAI=1).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a491d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation configuration:\n",
      "  SAMPLE_LIMIT: FULL\n",
      "  MAX_CONTEXTS_PER_ANSWER: 8\n",
      "  CONTEXT_TRUNC_CHARS: 800\n",
      "  TIMEOUT_SECONDS: 180\n",
      "  MAX_RETRIES: 3\n",
      "  BATCH_SIZE: 4\n",
      "  AUTO_FALLBACK_OPENAI: True\n",
      "  SEQUENTIAL_MODE: False\n",
      "  PROGRESS_INTERVAL: 10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataFrame 'df' not found. Run the data fetch cell first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Guard: ensure df exists from previous cell\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataFrame \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m not found. Run the data fetch cell first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Create working dataframe (sample or full)\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m EVAL_SAMPLE_LIMIT > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m EVAL_SAMPLE_LIMIT < \u001b[38;5;28mlen\u001b[39m(df):\n",
      "\u001b[31mRuntimeError\u001b[39m: DataFrame 'df' not found. Run the data fetch cell first."
     ]
    }
   ],
   "source": [
    "# Run Evaluation - Sequential Mode (Simplified)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Runs evaluation one row at a time to avoid rate limits and ensure progress is saved.\n",
    "# -----------------------------------------------------------------------------\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate as ragas_evaluate\n",
    "\n",
    "# Configuration\n",
    "EVAL_SAMPLE_LIMIT = int(os.getenv(\"RAGAS_SAMPLE_LIMIT\", \"0\"))          # 0 => use full dataset\n",
    "MAX_CONTEXTS_PER_ANSWER = int(os.getenv(\"RAGAS_MAX_CONTEXTS\", \"8\"))    # limit contexts per answer\n",
    "CONTEXT_TRUNC_CHARS = int(os.getenv(\"RAGAS_CONTEXT_TRUNC\", \"800\"))     # truncate each context to N chars\n",
    "TIMEOUT_SECONDS = int(os.getenv(\"RAGAS_TIMEOUT\", \"180\"))               # timeout per row\n",
    "MAX_RETRIES = int(os.getenv(\"RAGAS_MAX_RETRIES\", \"3\"))\n",
    "AUTO_FALLBACK_OPENAI = os.getenv(\"AUTO_FALLBACK_OPENAI\", \"1\") == \"1\"\n",
    "\n",
    "print(\"Evaluation configuration (Sequential):\")\n",
    "print(f\"  SAMPLE_LIMIT: {EVAL_SAMPLE_LIMIT if EVAL_SAMPLE_LIMIT>0 else 'FULL'}\")\n",
    "print(f\"  TIMEOUT_SECONDS: {TIMEOUT_SECONDS}\")\n",
    "\n",
    "# Guard: ensure df exists\n",
    "if 'df' not in globals():\n",
    "    raise RuntimeError(\"DataFrame 'df' not found. Please run the 'Connect to Database and Fetch Data' cell above first.\")\n",
    "\n",
    "# Prepare Data\n",
    "if EVAL_SAMPLE_LIMIT > 0 and EVAL_SAMPLE_LIMIT < len(df):\n",
    "    working_df = df.sample(n=EVAL_SAMPLE_LIMIT, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    working_df = df.copy().reset_index(drop=True)\n",
    "\n",
    "print(f\"Processing {len(working_df)} rows...\")\n",
    "\n",
    "# Sanitization\n",
    "sanitized_records = []\n",
    "for _, r in working_df.iterrows():\n",
    "    q = r.get(\"question\", \"\") or \"\"\n",
    "    a = r.get(\"answer\", \"\") or \"\"\n",
    "    ctxs = r.get(\"contexts\", []) or []\n",
    "    # Limit contexts and truncate\n",
    "    ctxs_clean = [c[:CONTEXT_TRUNC_CHARS] for c in ctxs[:MAX_CONTEXTS_PER_ANSWER] if isinstance(c, str) and c.strip()]\n",
    "    sanitized_records.append({\"question\": q, \"answer\": a, \"contexts\": ctxs_clean})\n",
    "\n",
    "sanitized_df = pd.DataFrame(sanitized_records)\n",
    "\n",
    "# Run Config\n",
    "try:\n",
    "    from ragas.run_config import RunConfig\n",
    "    run_config = RunConfig(timeout=TIMEOUT_SECONDS, max_retries=MAX_RETRIES)\n",
    "except ImportError:\n",
    "    run_config = None\n",
    "    print(\"‚ö†Ô∏è RunConfig not available\")\n",
    "\n",
    "# Sequential Evaluation Loop\n",
    "results_list = []\n",
    "print(\"\\nüîÑ Starting Sequential Evaluation...\")\n",
    "\n",
    "_t0 = time.time()\n",
    "\n",
    "for idx, row in sanitized_df.iterrows():\n",
    "    print(f\"Evaluating row {idx+1}/{len(sanitized_df)}...\", end=\" \", flush=True)\n",
    "    \n",
    "    # Create single-row dataset\n",
    "    single_ds = Dataset.from_pandas(pd.DataFrame([row]))\n",
    "    \n",
    "    try:\n",
    "        # Try primary judge\n",
    "        result = ragas_evaluate(\n",
    "            dataset=single_ds,\n",
    "            metrics=metrics,\n",
    "            llm=llm,\n",
    "            embeddings=embeddings,\n",
    "            raise_exceptions=True,\n",
    "            run_config=run_config,\n",
    "        )\n",
    "        print(\"‚úÖ Done\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {e}\")\n",
    "        # Fallback logic\n",
    "        if AUTO_FALLBACK_OPENAI and 'openai_judge_wrapper' in globals() and openai_judge_wrapper is not None:\n",
    "            print(\"   Attempting fallback to OpenAI...\", end=\" \", flush=True)\n",
    "            try:\n",
    "                result = ragas_evaluate(\n",
    "                    dataset=single_ds,\n",
    "                    metrics=metrics,\n",
    "                    llm=openai_judge_wrapper,\n",
    "                    embeddings=embeddings,\n",
    "                    raise_exceptions=True,\n",
    "                    run_config=run_config,\n",
    "                )\n",
    "                print(\"‚úÖ Recovered\")\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Fallback failed: {e2}\")\n",
    "                result = None\n",
    "        else:\n",
    "            result = None\n",
    "\n",
    "    if result:\n",
    "        res_df = result.to_pandas()\n",
    "        # Add back original ID\n",
    "        res_df['answer_id'] = working_df.iloc[idx]['answer_id']\n",
    "        results_list.append(res_df)\n",
    "    else:\n",
    "        # Add a row with NaN metrics if failed, to keep track\n",
    "        failed_row = row.to_dict()\n",
    "        failed_row['answer_id'] = working_df.iloc[idx]['answer_id']\n",
    "        results_list.append(pd.DataFrame([failed_row]))\n",
    "\n",
    "if results_list:\n",
    "    results_df = pd.concat(results_list, ignore_index=True)\n",
    "    \n",
    "    # Calculate aggregate score\n",
    "    metric_cols = [m.name for m in metrics]\n",
    "    # Filter to only columns that exist in results_df\n",
    "    existing_metric_cols = [c for c in metric_cols if c in results_df.columns]\n",
    "    \n",
    "    if existing_metric_cols:\n",
    "        results_df['llm_score_computed'] = results_df[existing_metric_cols].mean(axis=1, skipna=True)\n",
    "    \n",
    "    print(f\"\\nCompleted in {time.time() - _t0:.1f}s\")\n",
    "    print(f\"Results shape: {results_df.shape}\")\n",
    "else:\n",
    "    print(\"No results generated.\")\n",
    "    results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export per-answer metric details to JSON for future analysis\n",
    "import json, os, time\n",
    "\n",
    "if 'results_df' in globals():\n",
    "    metrics_export_cols = [\n",
    "        'answer_id', 'question', 'answer',\n",
    "        'faithfulness', 'answer_relevancy', 'llm_context_precision_without_reference',\n",
    "        'llm_score_computed'\n",
    "    ]\n",
    "    missing = [c for c in metrics_export_cols if c not in results_df.columns]\n",
    "    if missing:\n",
    "        print(f\"‚ö†Ô∏è Missing expected columns, cannot export all metrics: {missing}\")\n",
    "    export_cols = [c for c in metrics_export_cols if c in results_df.columns]\n",
    "    export_records = results_df[export_cols].to_dict(orient='records')\n",
    "    out_dir = os.path.join(os.getcwd(), 'data')\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ts = time.strftime('%Y%m%d_%H%M%S')\n",
    "    out_path = os.path.join(out_dir, f'ragas_metrics_{len(results_df)}_{ts}.json')\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_records, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ Saved metrics JSON: {out_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è results_df not found; run evaluation cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec04819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write LLM Scores Back to Database\n",
    "# Update the Answer table with computed LLM scores\n",
    "\n",
    "if 'llm_score_computed' in results_df.columns:\n",
    "    try:\n",
    "        conn = psycopg2.connect(DB_URL)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        updated_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"UPDATING DATABASE WITH LLM SCORES:\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        for idx, row in results_df.iterrows():\n",
    "            answer_id = row['answer_id']\n",
    "            llm_score = row['llm_score_computed']\n",
    "            \n",
    "            # Skip if score is NaN (evaluation failed)\n",
    "            if pd.isna(llm_score):\n",
    "                print(f\"‚ö†Ô∏è  Skipping {answer_id}: LLM score is NaN (evaluation failed)\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Update the llmScore field in the Answer table\n",
    "            update_query = \"\"\"\n",
    "                UPDATE \"Answer\"\n",
    "                SET \"llmScore\" = %s\n",
    "                WHERE id = %s\n",
    "            \"\"\"\n",
    "            cur.execute(update_query, (float(llm_score), answer_id))\n",
    "            print(f\"‚úÖ Updated {answer_id}: llmScore = {llm_score:.3f}\")\n",
    "            updated_count += 1\n",
    "        \n",
    "        conn.commit()\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úÖ Successfully updated {updated_count} Answer records with LLM scores\")\n",
    "        if skipped_count > 0:\n",
    "            print(f\"‚ö†Ô∏è  Skipped {skipped_count} records due to NaN scores\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error updating database: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        if 'conn' in locals():\n",
    "            conn.rollback()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No llm_score_computed column found. Skipping database update.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
